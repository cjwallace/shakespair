{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train a character LSTM recurrent neural network to generate Shakespearean text. We'll need some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need some data. I prepared some with some simple command line tools, there's a Makefile for it. If we were building this for production and we'd have to deal with changing input files, I'd definitely want to bring the data cleaning into python and test it. As it is, we can take the data file as given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open('../data/sentences.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to turn this list of sentences into suitable training data for a character LSTM. Our input will be a sequence of `n_char` characters, and the corresponding output will be the next character. To handle inputs shorter than `n_char` characters, we'll front pad the sequences with a `$` symbol, since this isn't used in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_char = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = [n_char * '$' + s for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [s[i : i + n_char]\n",
    "                   for s in padded_sentences\n",
    "                   for i in range(n_char)]\n",
    "\n",
    "output_characters = [s[i + n_char : i + n_char + 1] # handle that s[n_char] might not exist by using range\n",
    "                     for s in padded_sentences\n",
    "                     for i in range(n_char)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras takes a 3-tuple input data shape for LSTM cells, with the shape:\n",
    "\n",
    "`(number of training examples, number of steps in LSTM, length of feature vector)`\n",
    "\n",
    "We need to vectorize and binarize our data and transform it to this shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_set = sorted(set(''.join(padded_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '$', \"'\", ',', '-', '.', ':', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(character_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {c:i for i,c in enumerate(character_set)}\n",
    "idx_to_char = {i:c for i,c in enumerate(character_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_sentences),\n",
    "              n_char,\n",
    "              len(character_set)),\n",
    "             dtype=np.bool)\n",
    "\n",
    "y = np.zeros((len(input_sentences), len(character_set)),\n",
    "             dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an LSTM-flavoured neural network. Parameters arrived at after some experimentation as giving reasonable results. Judging the naturalness of the output of any sort of generative model for content is typically better done by a human than a loss function in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, return_sequences=True,\n",
    "         input_shape=(n_char, len(character_set))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(256),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(character_set)),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of generative content benefits from a human eye, it'd be nice to see how training is progressing as it does. We can define a callback function to be run at the end of each epoch to show us some generated text. First, we'll have to write the sampling function to actually get the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically by analogy to Boltzmann sampling, we introduce a \"temperature\" to the sample to control the diversity of characters generated. See [this paper](https://arxiv.org/pdf/1503.02531.pdf) for some details."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sample(predictions, temperature=1.0):\n",
    "    \"\"\"Sample a character from the output layer of the network.\n",
    "        generates more diverse output for lower values.\"\"\"\n",
    "    # avoiding underflow\n",
    "    preds = np.asarray([max(x,10**-10) for x in predictions]).astype('float64')\n",
    "    temp_preds = np.log(preds) / temperature\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"This function is invoked at the end of each epoch.\n",
    "       It prints some sample text generated by the network at\n",
    "       its current epoch, and writes the resulting model file\n",
    "       to disk.\"\"\"\n",
    "    print()\n",
    "    print(\"Finished training epoch %d\" % epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=128,\n",
    "          epochs=200,\n",
    "          callbacks=LambdaCallback(on_epoch_end=on_epoch_end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
