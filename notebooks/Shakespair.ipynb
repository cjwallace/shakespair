{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many others, I was duly impressed by Andrej Kaparthy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Here, we create a character level LSTM to model Shakespeare, and use it to build a sentence completer. Credit is due to the aforementioned blog post, and Robin Sloan's awesome assisted sci-fi writing project [Writing With the Machine](https://www.robinsloan.com/notes/writing-with-the-machine/) for the idea. The implementation closely follows the Keras [LSTM text generation example](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py) - this shouldn't really be considered novel code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train a character LSTM recurrent neural network to generate Shakespearean text. We'll need some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need some data. I prepared some with some simple command line tools, there's a Makefile for it. If we were building this for production and we'd have to deal with changing input files, I'd definitely want to bring the data cleaning into python and test it. As it is, we can take the data file as given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open('../data/sentences.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to turn this list of sentences into suitable training data for a character LSTM. Our input will be a sequence of `n_char` characters, and the corresponding output will be the next character. To handle inputs shorter than `n_char` characters, we'll front pad the sequences with a `$` symbol, since this isn't used in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_char = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = [n_char * '$' + s for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [s[i : i + n_char]\n",
    "                   for s in padded_sentences\n",
    "                   for i in range(len(s) - n_char)]\n",
    "\n",
    "output_characters = [s[i + n_char : i + n_char + 1] # handle that s[n_char] might not exist by using range\n",
    "                     for s in padded_sentences\n",
    "                     for i in range(len(s) - n_char)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(input_sentences) == len(output_characters)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# we should take a look at this to make sure we got the range logic right.\n",
    "input_sentences[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras takes a 3-tuple input data shape for LSTM cells, with the shape:\n",
    "\n",
    "`(number of training examples, number of steps in LSTM, length of feature vector)`\n",
    "\n",
    "We need to vectorize and binarize our data and transform it to this shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_set = sorted(set(''.join(padded_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '$', \"'\", ',', '-', '.', ':', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(character_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are for convenience.\n",
    "# We'll need to convert characters to indices and vice versa later.\n",
    "char_to_idx = {c:i for i,c in enumerate(character_set)}\n",
    "idx_to_char = {i:c for i,c in enumerate(character_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize zero arrays for vectorized input and output.\n",
    "X = np.zeros((len(input_sentences),\n",
    "              n_char,\n",
    "              len(character_set)),\n",
    "             dtype=np.bool)\n",
    "\n",
    "y = np.zeros((len(input_sentences), len(character_set)),\n",
    "             dtype=np.bool)\n",
    "\n",
    "# Fill the relevant indices with ones.\n",
    "# This routine will take a few seconds to run.\n",
    "for i, sentence in enumerate(input_sentences):\n",
    "    for t, character in enumerate(sentence):\n",
    "        X[i, t, char_to_idx[character]] = 1\n",
    "    y[i, char_to_idx[output_characters[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.shape[2] == y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an LSTM-flavoured neural network. The architecture is slightly simpler than that in Andrej Karpathy's blog post, with some added dropout since it seemed reasonable, after some experimentation. This was largely a training time reduction technique - we could certainly increase the capacity of the model, and probably get better results for it. Judging the naturalness of the output of any sort of generative model for content is typically better done by a human than a loss function in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, return_sequences=True,\n",
    "         input_shape=(n_char, len(character_set))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(256),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(character_set)),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of generative content benefits from a human eye, it'd be nice to see how training is progressing as it does. We can define a callback function to be run at the end of each epoch to show us some generated text. First, we'll have to write the sampling function to actually get the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically by analogy to Boltzmann sampling, we introduce a \"temperature\" to the sample to control the diversity of characters generated. See [this paper](https://arxiv.org/pdf/1503.02531.pdf) for some details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(predictions, temperature=1.0):\n",
    "    \"\"\"Sample a character from the output layer of the network.\n",
    "       Generates more diverse output for lower values of temperature.\"\"\"\n",
    "    # avoiding underflow\n",
    "    p = np.asarray([max(x,10**-10) for x in predictions]).astype('float64')\n",
    "    p = np.log(p) / temperature\n",
    "    p = np.exp(p) / np.sum(np.exp(p))\n",
    "    probs = np.random.multinomial(1, p)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"This function is invoked at the end of each epoch.\n",
    "       It prints some sample text generated by the network at\n",
    "       its current epoch, and writes the resulting model file\n",
    "       to disk.\"\"\"\n",
    "    print()\n",
    "    print(\"Finished training epoch %d\" % epoch)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    # seed the sentence with padding\n",
    "    seed = n_char * '$'\n",
    "    generated += seed\n",
    "    last_char = generated[-1:]\n",
    "    \n",
    "    # define characters that end sentences\n",
    "    stop_chars = ['.','?','!']\n",
    "    while last_char not in stop_chars:\n",
    "        x_test = np.zeros((1, n_char, len(character_set)))\n",
    "        for t, character in enumerate(seed):\n",
    "            x_test[0, t, char_to_idx[character]] = 1\n",
    "        \n",
    "        out = model.predict(x_test)[0]\n",
    "        pred_idx = sample(out)\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        \n",
    "        generated += pred_char\n",
    "        seed = generated[-n_char:]\n",
    "        last_char = generated[-1:]\n",
    "        \n",
    "    # remove padding characters from generated text\n",
    "    print(generated.replace('$',''))\n",
    "    model.save('../data/epoch_' + str(epoch) + '.h5')\n",
    "    #print(\"Generated text:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to fit the model. This is costly, in both time and, if you're using a paid-for cloud GPU, money. We've set a large batch size to speed up each epoch, though you would probably get better results with smaller batches (which may permit fewer epochs to see good results, I have not experimented much here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=2048,\n",
    "          epochs=60,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
